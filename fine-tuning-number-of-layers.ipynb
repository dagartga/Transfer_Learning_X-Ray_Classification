{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport re\nimport glob\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nprint(tf.__version__)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path()\nBATCH_SIZE = 64*strategy.num_replicas_in_sync\n#BATCH_SIZE = 64 # for non-TPU use\nIMAGE_SIZE = [180, 180]\nEPOCHS = 50","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames = tf.io.gfile.glob(str(GCS_PATH + '/chest_xray/train/*/*'))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH + '/chest_xray/val/*/*')))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH + '/chest_xray/test/*/*')))\n\nprint(f'The total number of files is {len(filenames)}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the data into 60:40 for train:test+val\ntrain_files, val_test_files = train_test_split(filenames, test_size=0.4,random_state=18)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the val_test_files to 50:50 to have a final split of 60:20:20 of train:val:test\nval_files, test_files = train_test_split(val_test_files, test_size=0.5, random_state=18)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pct_df = pd.DataFrame(columns = ['data_set', 'label', 'pct'], index = range(6))\n\ntotals = {'train':len(train_files), 'val':len(val_files), 'test':len(test_files)}\n\nval_pneum_count = 0\nval_norm_count = 0\n\nre_pattern = '.*/(NORMAL|PNEUMONIA)/.*'\n\nfor file in val_files:\n    re_match = re.match(re_pattern, file)\n    if re_match[1] == 'NORMAL':\n        val_norm_count += 1\n    else:\n        val_pneum_count += 1\n\npct_df.iloc[0, :] = ['val', 'NORMAL', round(100* val_norm_count / totals['val'], 1)]\npct_df.iloc[1, :] = ['val', 'PNEUMONIA', round(100* val_pneum_count / totals['val'], 1)]\n\n\ntest_pneum_count = 0\ntest_norm_count = 0\nfor file in test_files:\n    re_match = re.match(re_pattern, file)\n    if re_match[1] == 'NORMAL':\n        test_norm_count += 1\n    else:\n        test_pneum_count += 1\n\npct_df.iloc[2, :] = ['test', 'NORMAL', round(100* test_norm_count / totals['test'], 1)]\npct_df.iloc[3, :] = ['test', 'PNEUMONIA', round(100* test_pneum_count / totals['test'], 1)]\n\ntrain_pneum_count = 0\ntrain_norm_count = 0\nfor file in train_files:\n    re_match = re.match(re_pattern, file)\n    if re_match[1] == 'NORMAL':\n        train_norm_count += 1\n    else:\n        train_pneum_count += 1\n\npct_df.iloc[4, :] = ['train', 'NORMAL', round(100* train_norm_count / totals['train'], 1)]\npct_df.iloc[5, :] = ['train', 'PNEUMONIA', round(100* train_pneum_count / totals['train'], 1)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PNEUM_COUNT = val_pneum_count + train_pneum_count + test_pneum_count\nNORMAL_COUNT = val_norm_count + train_norm_count + test_norm_count\nprint(f'Normal: {NORMAL_COUNT}\\nPneumonia: {PNEUM_COUNT}')\n\npct_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_list_ds = tf.data.Dataset.from_tensor_slices(train_files)\nval_list_ds = tf.data.Dataset.from_tensor_slices(val_files)\ntest_list_ds = tf.data.Dataset.from_tensor_slices(test_files)\n\n#view the file paths for training\nprint('----Training Files----')\nfor f in train_list_ds.take(5):\n    print(f.numpy())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_IMG_COUNT = tf.data.experimental.cardinality(train_list_ds).numpy()\nVAL_IMG_COUNT = tf.data.experimental.cardinality(val_list_ds).numpy()\nTEST_IMG_COUNT = tf.data.experimental.cardinality(test_list_ds).numpy()\n\nprint(f'Train Image Count: {TRAIN_IMG_COUNT}\\nVal Image Count: {VAL_IMG_COUNT}\\nTest Image Count: {TEST_IMG_COUNT}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# will label as 1 if pneumonia otherwise 0\ndef get_label(file_path):\n    parts = tf.strings.split(file_path, os.path.sep)\n    return parts[-2] == 'PNEUMONIA'\n\ndef decode_img(img):\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    return tf.image.resize(img, IMAGE_SIZE)\n\ndef process_path(file_path):\n    label = get_label(file_path)\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    return img, label","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n\nval_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n\ntest_ds = test_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = []\nfor image, label in train_ds.take(TRAIN_IMG_COUNT):\n    y_train.append(label)\n    \nprint('Training Counts')\nprint(f'Pneumonia count: {np.sum(y_train)}\\nNormal count: {TRAIN_IMG_COUNT - np.sum(y_train)}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val = []\nfor image, label in val_ds.take(VAL_IMG_COUNT):\n    y_val.append(label)\n    \nprint('Validation Counts')\nprint(f'Pneumonia count: {np.sum(y_val)}\\nNormal count: {VAL_IMG_COUNT - np.sum(y_val)}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = []\nfor image, label in test_ds.take(TEST_IMG_COUNT):\n    y_test.append(label)\n    \nprint('Test Counts')\nprint(f'Pneumonia count: {np.sum(y_test)}\\nNormal count: {TEST_IMG_COUNT - np.sum(y_test)}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n    \n    if cache:\n        if isinstance(cache, str):\n            ds = ds.cache(cache)\n        else:\n            ds = ds.cache()\n            \n    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n    \n    ds = ds.repeat()\n    \n    ds = ds.batch(BATCH_SIZE)\n    \n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    \n    return ds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = prepare_for_training(train_ds)\nval_ds = prepare_for_training(val_ds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weight_for_0 = (1 / NORMAL_COUNT)*(TRAIN_IMG_COUNT)/2.0\nweight_for_1 = (1 / PNEUM_COUNT)*(TRAIN_IMG_COUNT)/2.0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weight = {0: weight_for_0, 1: weight_for_1}\nprint(f'Weight for class 0: {weight_for_0:.2f}')\nprint(f'Weight for class 1: {weight_for_1:.2f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create some metrics for comparing on test results\n\ndef precision(y_test, y_pred):\n    tp, fp = 0, 0\n    \n    for i in range(len(y_test)):\n        if y_test[i] >= 0.5 and y_pred[i] >= 0.5:\n            tp += 1\n        elif y_test[i] < 0.5 and y_pred[i] >= 0.5:\n            fp += 1\n            \n    return tp / (tp + fp)\n\ndef recall(y_test, y_pred):\n    tp, fn = 0, 0\n    \n    for i in range(len(y_test)):\n        if y_test[i] >= 0.5 and y_pred[i] >= 0.5:\n            tp += 1\n        elif y_test[i] >= 0.5 and y_pred[i] < 0.5:\n            fn += 1\n            \n    return tp / (tp + fn)\n\ndef f1_score(y_test, y_pred):\n    numerator = precision(y_test, y_pred) * recall(y_test, y_pred)\n    denominator = precision(y_test, y_pred) + recall(y_test, y_pred)\n    \n    return 2 * (numerator / denominator)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine Tune the Model\n\n## Expore different number of Dense Layers","metadata":{}},{"cell_type":"code","source":"# add exponential decay to try to improve fine tuning\ndef exponential_decay(lr0, s):\n    def exponential_decay_fn(epoch):\n        return lr0 * 0.1 ** (epoch / s)\n    return exponential_decay_fn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use learning rate that gave best results in the learning rate hyperparameter tuning\nlearning_rate = 0.06103\n# use hidden_units that gave best results in the hidden units hyperparameter tuning\nhidden_units = 4096\n# use mini batch size that gave best results in the mini batch hyperparameter tuning\nmini_batch = 32\n\n# try 10 different layer combinations\nfor num_of_layers in range(8):\n    \n    BATCH_SIZE = mini_batch*strategy.num_replicas_in_sync\n    print(f'\\n\\nBatch Size: {BATCH_SIZE}')    \n    print(f'Hidden Units: {hidden_units}')\n    print(f'Learning Rate: {learning_rate}')\n    \n    \n    with strategy.scope():\n        \n        base_model = tf.keras.applications.densenet.DenseNet201(\n                        input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n                        include_top=False,\n                        weights='imagenet'\n                        )\n        base_model.trainable = False\n        \n        if num_of_layers < 4:\n        \n            x=base_model.output\n            inputs = tf.keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n            x = base_model(inputs, training=False)\n            x = tf.keras.layers.GlobalAveragePooling2D()(x)\n            x = tf.keras.layers.Dropout(0.3)(x)\n            \n            for i in range(num_of_layers):\n                \n                hidden_units_updated = 1 / (i + 1)\n                print(f'Dense Layer {i}: {hidden_units_updated}')\n                \n                x = tf.keras.layers.Dense(hidden_units_updated, activation='relu')(x)\n                x = tf.keras.layers.BatchNormalization()(x)\n                x = tf.keras.layers.Dropout(0.3)(x)\n                \n            outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n            model = tf.keras.Model(inputs, outputs)\n    \n        if num_of_layers >= 4:\n        \n            x=base_model.output\n            inputs = tf.keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n            x = base_model(inputs, training=False)\n            x = tf.keras.layers.GlobalAveragePooling2D()(x)\n            x = tf.keras.layers.Dropout(0.3)(x)\n            x = tf.keras.layers.Dense(hidden_units, activation='relu')(x)\n            x = tf.keras.layers.BatchNormalization()(x)\n            x = tf.keras.layers.Dropout(0.3)(x)\n            print(f'Dense layer 1: {hidden_units})\n            \n            for i in range(num_of_layers):\n                \n                hidden_units_updated = 1 / (i + 1)\n                print(f'Dense Layer {i+1}: {hidden_units_updated}')\n                \n                x = tf.keras.layers.Dense(hidden_units_updated, activation='relu')(x)\n                x = tf.keras.layers.BatchNormalization()(x)\n                x = tf.keras.layers.Dropout(0.3)(x)\n                \n            outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n            model = tf.keras.Model(inputs, outputs)\n        \n        \n        METRICS = [\n                'accuracy',\n                tf.keras.metrics.Precision(name='precision'),\n                tf.keras.metrics.Recall(name='recall')\n                ]\n        \n        model.compile(\n                    optimizer='adam',\n                    loss='binary_crossentropy',\n                    metrics=METRICS\n                     )\n            \n        checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(f\"densenet_{learning_rate}_{hidden_units}_{BATCH_SIZE}_xray_model.h5\", save_best_only=True)\n        early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n        exponential_decay_fn = exponential_decay(learning_rate, 20)\n        lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n    \n        history = model.fit(\n                            train_ds,\n                            steps_per_epoch=TRAIN_IMG_COUNT // BATCH_SIZE,\n                            epochs=EPOCHS,\n                            validation_data=val_ds,\n                            validation_steps=VAL_IMG_COUNT // BATCH_SIZE,\n                            class_weight=class_weight,\n                            callbacks=[checkpoint_cb, early_stopping_cb, lr_scheduler]\n                            )","metadata":{},"execution_count":null,"outputs":[]}]}